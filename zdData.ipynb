{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9428edf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from transformers import BertTokenizer\n",
    "from PIL import Image, ImageOps\n",
    "import scipy.io\n",
    "#import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed3dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex font\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "#matplotlib.use('PS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882549ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "amt_data_dir = \"/home/zdai/repos/EgoTransformer/Data/amt_data/\"\n",
    "ana_file = join('Data', 'deepdiary_data', 'lifelog', 'dataset.json')\n",
    "vocab_file = join('Data', 'data', 'vocabulary.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281978a",
   "metadata": {},
   "source": [
    "## Deepdiary Image-Caption pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "amt_data_dir = \"/Users/zhuangzhuangdai/repos/EgoTransformer/images/amt_data\"\n",
    "ana_file = join(amt_data_dir, 'amt_list.txt')\n",
    "#vocab_file = join('Data', 'data', 'vocabulary.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ana_file, \"r\") as file:\n",
    "    ana = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20cac46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ana['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_305 = []\n",
    "imgids = []\n",
    "\n",
    "for img_name in os.listdir(amt_data_dir):\n",
    "    for img_dict in ana['images']:\n",
    "        if img_name == img_dict['filename']:\n",
    "            img_305.append(img_dict)\n",
    "            imgids.append(img_dict['imgid'])\n",
    "            \n",
    "print(len(imgids), len(img_305))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22cdad7",
   "metadata": {},
   "source": [
    "### Use 'amt_list.txt' directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20016eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ana_file, \"r\") as file:\n",
    "    pairs_str = file.read()\n",
    "    pairs = pairs_str.split('\\n')[1:]\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = {}\n",
    "\n",
    "for pair in pairs:\n",
    "    img_name = pair.split(' ')[0]\n",
    "    sent_ana = pair.split('.jpg ')[-1]\n",
    "    \n",
    "    if img_name not in img_names:\n",
    "        img_names[img_name] = [sent_ana]\n",
    "    else:\n",
    "        img_names[img_name].append(sent_ana)\n",
    "        \n",
    "len(img_names.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_idx = 50\n",
    "for idx, (key, val) in enumerate(img_names.items()):\n",
    "    if idx == my_idx:\n",
    "        img_name0, cap_lst0 = key, val\n",
    "        break\n",
    "\n",
    "image0 = Image.open(join(amt_data_dir, img_name0))\n",
    "# Transpose with respect to EXIF data\n",
    "image0 = ImageOps.exif_transpose(image0)\n",
    "w, h = image0.size\n",
    "print(\"PIL Image width: {}, height: {}\".format(w, h))\n",
    "\n",
    "plt.imshow(image0)\n",
    "plt.show()\n",
    "print(cap_lst0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deepdiary(index, ann_filename):\n",
    "    with open(ann_filename, \"r\") as file:\n",
    "        pairs_str = file.read()\n",
    "        pairs = pairs_str.split('\\n')[1:]\n",
    "    \n",
    "    img_names = {}\n",
    "\n",
    "    for pair in pairs:\n",
    "        img_name = pair.split(' ')[0]\n",
    "        sent_ana = pair.split('.jpg ')[-1]\n",
    "\n",
    "        if img_name not in img_names:\n",
    "            img_names[img_name] = [sent_ana]\n",
    "        else:\n",
    "            img_names[img_name].append(sent_ana)\n",
    "    \n",
    "    for idx, (key, val) in enumerate(img_names.items()):\n",
    "        if idx == index:\n",
    "            img_name0, cap_lst0 = key, val\n",
    "            break\n",
    "\n",
    "    image0 = Image.open(join(amt_data_dir, img_name0))\n",
    "    # Transpose with respect to EXIF data\n",
    "    image0 = ImageOps.exif_transpose(image0)\n",
    "    w, h = image0.size\n",
    "    print(\"PIL Image width: {}, height: {}\".format(w, h))\n",
    "\n",
    "    plt.imshow(image0)\n",
    "    plt.show()\n",
    "    print(cap_lst0)\n",
    "    return image0, cap_lst0, img_name0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacff1b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuples = get_deepdiary(50, ana_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcc13f",
   "metadata": {},
   "source": [
    "## Deepdiary VGG16 features & caption pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86377bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_data_dir = \"/home/zdai/repos/EgoTransformer/Data/deepdiary_data/lifelog\"\n",
    "diary_ann = join(diary_data_dir, 'dataset.json')\n",
    "diary_data = join(diary_data_dir, 'vgg_feats.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0870cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diary_mat = scipy.io.loadmat(diary_data)\n",
    "diary_mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79478963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diary_mat['feats'][:50, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_mat['feats'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a30acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(diary_mat['feats'][:, -7] == diary_mat['feats'][:, -9]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de95f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(diary_ann, \"r\") as file:\n",
    "    diary_anns = json.load(file)\n",
    "len(diary_anns['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_anns['images'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9047e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_pairs = diary_anns['images'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "ann_pairs = []\n",
    "last_sent0, last_sent_1 = '', ''\n",
    "\n",
    "for idx, item in enumerate(diary_anns['images']):\n",
    "    \n",
    "    if item['sentences'][0]['raw'] == last_sent0 and item['sentences'][-1]['raw'] == last_sent_1:\n",
    "        diary_pairs[idx]['segment'] = index\n",
    "        \n",
    "        tup[0].append(diary_mat['feats'][:, idx])\n",
    "        \n",
    "    else:\n",
    "        last_sent0 = item['sentences'][0]['raw']\n",
    "        last_sent_1 = item['sentences'][-1]['raw']\n",
    "        \n",
    "        if idx != 0:\n",
    "            ann_pairs.append(tup)\n",
    "            del tup\n",
    "        \n",
    "        index += 1\n",
    "        diary_pairs[idx]['segment'] = index\n",
    "        \n",
    "        # tuple of (vgg_feature_vec, [captions])\n",
    "        caps = [sent['raw'] for sent in item['sentences']]\n",
    "        tup = ([diary_mat['feats'][:, idx]], caps)\n",
    "    \n",
    "    diary_pairs[idx]['feats'] = diary_mat['feats'][:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b85355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ann_pairs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c727191",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(diary_data_dir, 'diary_pairs.pkl'), 'wb') as a_file:\n",
    "    pickle.dump(diary_pairs, a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading ###\n",
    "with open(join(diary_data_dir, 'diary_pairs.pkl'), 'rb') as a_file:\n",
    "    diary_pairs = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(diary_data_dir, 'ann_pairs.pkl'), 'wb') as a_file:\n",
    "    pickle.dump(ann_pairs, a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493129be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading ###\n",
    "with open(join(diary_data_dir, 'ann_pairs.pkl'), 'rb') as a_file:\n",
    "    ann_pairs = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pairs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4f9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d12c5e9",
   "metadata": {},
   "source": [
    "## MSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "msvd_data_dir = \"/home/zdai/repos/MSVD\"\n",
    "msvd_ana_file = join(msvd_data_dir, 'AllVideoDescriptions.txt')\n",
    "skipped_dir = join(msvd_data_dir, 'skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27663c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "with open(msvd_ana_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    print(\"Num of lines = \", len(lines))\n",
    "    for line in lines:\n",
    "        if line != \"\\n\" and line[0] != \"#\":\n",
    "            pairs.append(line)\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62030b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_anns = {}\n",
    "min_frame_per_clip = 5\n",
    "\n",
    "for pair in pairs:\n",
    "    img_name = pair.split(' ')[0]\n",
    "    sent_ana = pair[len(img_name) + 1:-1]\n",
    "    \n",
    "    if img_name not in vid_anns:\n",
    "        # Create a list of [path_to_image]\n",
    "        img_keys = []\n",
    "        # Discard clip with less than N frames\n",
    "        if len(os.listdir(join(skipped_dir, img_name))) < min_frame_per_clip:\n",
    "            continue\n",
    "        \n",
    "        for frame in sorted(os.listdir(join(skipped_dir, img_name)), key=lambda x: int(x.split('.')[0][6:])):\n",
    "            #print(frame)\n",
    "            img_keys.append(join(skipped_dir, img_name, frame))\n",
    "        #show_img(img_keys[0])\n",
    "        \n",
    "        vid_anns[img_name] = (img_keys, [sent_ana])\n",
    "    else:\n",
    "        if sent_ana in vid_anns[img_name][1]:\n",
    "            continue\n",
    "        vid_anns[img_name][1].append(sent_ana)\n",
    "        \n",
    "len(vid_anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc20357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(path_to_img):\n",
    "    image0 = Image.open(path_to_img)\n",
    "    # Transpose with respect to EXIF data\n",
    "    image0 = ImageOps.exif_transpose(image0)\n",
    "    w, h = image0.size\n",
    "    print(\"PIL Image width: {}, height: {}\".format(w, h))\n",
    "\n",
    "    plt.imshow(image0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39677814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vid_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anns = []\n",
    "\n",
    "for idx, (key, val) in enumerate(vid_anns.items()):\n",
    "    for index in range(len(val[1])):\n",
    "        for i in range(len(val[0]) - min_frame_per_clip + 1):\n",
    "            tuple_item = (val[0][i:i + min_frame_per_clip], val[1][index])\n",
    "            Anns.append(tuple_item)\n",
    "\n",
    "len(Anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exhaustive selection\n",
    "import random\n",
    "window_frame_per_clip = 5\n",
    "Anns_train, Anns_test = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30892dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(list(vid_anns.keys()), test_size=0.3, random_state=123, shuffle=True)\n",
    "\n",
    "for idx, (key, val) in enumerate(vid_anns.items()):\n",
    "    \n",
    "    for i in range(len(val[0]) - window_frame_per_clip + 1):\n",
    "        \n",
    "        tuple_item = (val[0][i:i + window_frame_per_clip], random.choice(val[1]))\n",
    "        # Split vid_anns based on whether vid_name in X_train/X_test lists\n",
    "        if key in X_train:\n",
    "            Anns_train.append(tuple_item)\n",
    "        elif key in X_test:\n",
    "            Anns_test.append(tuple_item)\n",
    "            \n",
    "        # Break to avoid sample too many from long clips\n",
    "        if i > 15:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2dcfc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(Anns_train), len(Anns_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36784737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(Anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4c1ff",
   "metadata": {},
   "source": [
    "### Load with Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cfcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_msvd(msvd_ana_file, skipped_dir, min_frame_per_clip=7, window_frame_per_clip=5):\n",
    "    # TODO: split MSVD into train / test\n",
    "    pairs, Anns_train, Anns_test = [], [], []\n",
    "    vid_anns = {}\n",
    "    \n",
    "    assert window_frame_per_clip <= min_frame_per_clip\n",
    "\n",
    "    with open(msvd_ana_file, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        print(\"Num of lines = \", len(lines))\n",
    "        for line in lines:\n",
    "            if line != \"\\n\" and line[0] != \"#\":\n",
    "                pairs.append(line)\n",
    "\n",
    "    for pair in pairs:\n",
    "        img_name = pair.split(' ')[0]\n",
    "        sent_ana = pair[len(img_name) + 1:-1]\n",
    "\n",
    "        if img_name not in vid_anns:\n",
    "            # Create a list of [path_to_image]\n",
    "            img_keys = []\n",
    "            # Discard clip with less than N frames\n",
    "            if len(os.listdir(join(skipped_dir, img_name))) < min_frame_per_clip:\n",
    "                continue\n",
    "\n",
    "            for frame in sorted(os.listdir(join(skipped_dir, img_name)), key=lambda x: int(x.split('.')[0][6:])):\n",
    "                img_keys.append(join(skipped_dir, img_name, frame))\n",
    "\n",
    "            vid_anns[img_name] = (img_keys, [sent_ana])\n",
    "        else:\n",
    "            if sent_ana in vid_anns[img_name][1]:\n",
    "                continue\n",
    "            vid_anns[img_name][1].append(sent_ana)\n",
    "    \n",
    "    # vid_anns is a dict of {'vid_name': (['img1.jpg', 'img2.jpg' ...], ['cap1', 'cap2' ...]), ... ...}\n",
    "    #print(next(iter(vid_anns)))\n",
    "    X_train, X_test = train_test_split(list(vid_anns.keys()), test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "    for idx, (key, val) in enumerate(vid_anns.items()):\n",
    "        for index in range(len(val[1])):\n",
    "            for i in range(len(val[0]) - window_frame_per_clip + 1):\n",
    "                tuple_item = (val[0][i:i + window_frame_per_clip], val[1][index])\n",
    "                # Split vid_anns based on whether vid_name in X_train/X_test lists\n",
    "                if key in X_train:\n",
    "                    Anns_train.append(tuple_item)\n",
    "                elif key in X_test:\n",
    "                    Anns_test.append(tuple_item)\n",
    "\n",
    "    return Anns_train, Anns_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e29b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "msvd_data_dir = \"/home/zdai/repos/MSVD\"\n",
    "msvd_ana_file = join(msvd_data_dir, 'AllVideoDescriptions.txt')\n",
    "skipped_dir = join(msvd_data_dir, 'skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bd1b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anns_train, anns_test = read_msvd(msvd_ana_file, skipped_dir, min_frame_per_clip=7, window_frame_per_clip=5)\n",
    "print(len(anns_train), len(anns_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c852cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# anns_ is a list of tuple (['path/to/img1.jpg', 'path/to/img2.jpg' ...], 'A dog is sitting')\n",
    "anns_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8034e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccdadd9",
   "metadata": {},
   "source": [
    "# EgoCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "egocap_dir = '/Users/zhuangzhuangdai/repos/EgoCapSurvey/doc'\n",
    "egocap_filename = 'analyzed_annatations.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0edec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(egocap_dir, egocap_filename), 'r') as f:\n",
    "    ana = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65263a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference labels\n",
    "ego4d_dir = '/Users/zhuangzhuangdai/ego4d_data'\n",
    "with open(join(ego4d_dir, 'ego4d.json')) as f:\n",
    "    ref_ego4d = json.load(f)\n",
    "    \n",
    "coco_dir = '/Users/zhuangzhuangdai/repos/EgoTransformer/images/annotations'\n",
    "with open(join(coco_dir, 'captions_val2017.json')) as f:\n",
    "    ref_coco = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c374096",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_coco['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7316503",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add reference labels from original {COCO, Ego4D, MSVD, MSRVTT} datasets ###\n",
    "def update_ann_reference(ann, ref, ref_co):\n",
    "    for idx, (key, val) in enumerate(ann.items()):\n",
    "        filename = None\n",
    "        try:\n",
    "            # For image name parsing\n",
    "            key_lst = key.split('.')[0].split('_')\n",
    "            if len(key_lst) == 1:\n",
    "                # COCO\n",
    "                filename = key_lst[0]\n",
    "                cap_lst = []\n",
    "                for video_item in ref_co['annotations']:\n",
    "                    if str(video_item['image_id']).zfill(12) == filename:\n",
    "                        cap_lst.append(video_item['caption'])\n",
    "                        \n",
    "                ann[key]['reference'] = {\n",
    "                            'origin': 'coco_val2017',\n",
    "                            'labels': cap_lst\n",
    "                        }\n",
    "                \n",
    "            elif len(key_lst) == 2:\n",
    "                # Ego4D\n",
    "                filename = key_lst[0]\n",
    "                \n",
    "                for video_item in ref['videos']:\n",
    "                    if video_item['video_uid'] == filename:\n",
    "                        labels = video_item['scenarios']\n",
    "                        ann[key]['reference'] = {\n",
    "                            'origin': 'Ego4D',\n",
    "                            'labels': labels\n",
    "                        }\n",
    "                        break\n",
    "            \n",
    "        except:\n",
    "            raise TypeError(\"Not an image with reference!\")\n",
    "            \n",
    "\n",
    "                \n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ana = update_ann_reference(ana, ref_ego4d, ref_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \"analyzed_annatations_ref\"\n",
    "with open(join(egocap_dir, 'analyzed_annatations_ref.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_ana, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f489d",
   "metadata": {},
   "source": [
    "# MODEL EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372e19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configuration import Config, ConfigEgo\n",
    "from Eval import predict_qualitative\n",
    "from pycocoevalcap.bleu.bleu import Bleu, BleuScorer\n",
    "from pycocoevalcap.meteor.meteor import Meteor, METEOR_JAR\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider, CiderScorer\n",
    "from pycocoevalcap.spice.spice import Spice, SPICE_JAR\n",
    "\n",
    "def calc_scores(ref, hypo):\n",
    "    \"\"\"\n",
    "    ref, dictionary of reference sentences (id, sentence)\n",
    "    hypo, dictionary of hypothesis sentences (id, sentence)\n",
    "    score, dictionary of scores\n",
    "    \"\"\"\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "        #(Meteor(), \"METEOR\"),\n",
    "        (Rouge(), \"ROUGE_L\"),\n",
    "        (Cider(), \"CIDEr\"),\n",
    "        #(Spice(), \"SPICE\")\n",
    "    ]\n",
    "    final_scores = {}\n",
    "    for scorer, method in scorers:\n",
    "        score, scores = scorer.compute_score(ref, hypo)\n",
    "        if type(score) == list:\n",
    "            for m, s in zip(method, score):\n",
    "                final_scores[m] = s\n",
    "        else:\n",
    "            final_scores[method] = score\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49274a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "egocap_dir = '/home/zdai/repos/EgoCapSurvey'\n",
    "#egocap_filename = 'analyzed_annatations_ref.json'\n",
    "egocap_filename = 'EgoCapAnnotations.json'\n",
    "\n",
    "with open(join(egocap_dir, 'doc', egocap_filename), 'r') as f:\n",
    "    ana = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70673eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2079"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c182d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8cda2388-6f45-46d4-8ded-4ba2e23b309f_small.jpg {'SplitIndex': 18, 'captions': ['I am walking outside.', 'I am walking in the park.', 'I am in a park on a sunny day.'], 'tags': {'where': ['outdoor', 'outdoor', 'outdoor'], 'what_activity': ['na', 'na', 'na'], 'when': ['daytime', 'daytime', 'daytime'], 'who': ['human', 'na', 'na']}, 'reported': 'False', 'cap_similarity': {'Bleu_1': 0.4548979945570264, 'Bleu_2': 0.4288819422480427, 'Bleu_3': 0.3820903725557863, 'Bleu_4': 7.626719270624179e-05, 'ROUGE_L': 0.5791139240506329, 'CIDEr': 0.0}, 'tag_stats': {'where': {'majority': 'outdoor', 'percentage': 1.0, 'total_options': 1}, 'what_activity': {'majority': 'na', 'percentage': 1.0, 'total_options': 1}, 'when': {'majority': 'daytime', 'percentage': 1.0, 'total_options': 1}, 'who': {'majority': 'na', 'percentage': 0.6666666666666666, 'total_options': 2}}, 'REPORTED': 'False', 'reference': {'origin': 'Ego4D', 'labels': ['Gaming arcade / pool / billiards', 'Talking with friends/housemates']}}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(ana)), ana[next(iter(ana))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd153468",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_t = Config()\n",
    "config_ego = ConfigEgo()\n",
    "\n",
    "baseline = 'finetuneBaseline-best_epoch22_loss12.pth'  #'finetune-best_epoch19_loss15.pth'\n",
    "egotrans = 'finetuneEgoTrans-best_epoch14_loss11.pth'  #'finetuneContextFuse-epoch99_loss16.pth'\n",
    "\n",
    "egotrans_freezebackbone = '9 - finetuneContextFuse-epoch54_loss15.pth'\n",
    "egotrans_freezeencoder = '10 - finetuneContextFuse-epoch19_loss15.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f755a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt image\n",
    "sample_name = '1a7b9b5d-499a-454e-aaee-3758718fb5f2_small.jpg'\n",
    "\n",
    "split_idx = str(ana[sample_name]['SplitIndex']).zfill(2)\n",
    "sample_path = join(egocap_dir, 'static', 'Split' + split_idx, sample_name)\n",
    "tags = (ana[sample_name]['tag_stats']['where']['majority'], ana[sample_name]['tag_stats']['when']['majority'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 9,
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   "id": "8fbd167e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint...\n",
<<<<<<< HEAD
      "Current checkpoint epoch = 22\n",
      "Initializing Device: cpu\n",
      "Loading pretrained Tokenizer takes: 11.47s\n",
=======
      "Current checkpoint epoch = 29\n",
      "Initializing Device: cpu\n",
      "Loading pretrained Tokenizer takes: 7.76s\n",
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
      "Total Vocal =  30522\n",
      "Start Token: [CLS]; End Token: [SEP]; Padding: [PAD]\n",
      "PIL Image width: 480, height: 360\n",
      "\n",
      "I am driving a car.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_dict = predict_qualitative(config_t, sample_path, tags=None, checkpoint_path=baseline)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 10,
   "id": "2e0a49d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1a7b9b5d-499a-454e-aaee-3758718fb5f2_small.jpg': ['i am driving a car.']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   "id": "fd1fe5cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint...\n",
      "Current checkpoint epoch = 14\n",
      "Initializing Device: cpu\n",
      "Loading pretrained Tokenizer takes: 12.89s\n",
      "Total Vocal =  30522\n",
      "Start Token: [CLS]; End Token: [SEP]; Padding: [PAD]\n",
      "PIL Image width: 480, height: 360\n",
      "\n",
      "I am driving a car.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cap_dict = predict_qualitative(config_ego, sample_path, tags=tags, checkpoint_path=egotrans)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 11,
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   "id": "8e404b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1a7b9b5d-499a-454e-aaee-3758718fb5f2_small.jpg': ['I am inside a car.', 'I am in a car in a garage.', 'I am sitting inside my car.', 'I am sitting inside a car.']}\n"
     ]
    }
   ],
   "source": [
    "gts = {sample_name: ana[sample_name]['captions']}\n",
    "print(gts)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 12,
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   "id": "037527b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "{'testlen': 5, 'reflen': 5, 'guess': [5, 4, 3, 2], 'correct': [4, 3, 1, 0]}\n",
      "ratio: 0.9999999998000002\n",
      "{'testlen': 5, 'reflen': 5, 'guess': [5, 4, 3, 2], 'correct': [4, 3, 1, 0]}\n",
      "ratio: 0.9999999998000002\n",
      "{'Bleu_1': 0.7999999996800004, 'Bleu_2': 0.7745966689122801, 'Bleu_3': 0.5848035473729143, 'Bleu_4': 9.999999994791673e-05, 'ROUGE_L': 0.6, 'CIDEr': 0.0} {'Bleu_1': 0.7999999996800004, 'Bleu_2': 0.7745966689122801, 'Bleu_3': 0.5848035473729143, 'Bleu_4': 9.999999994791673e-05, 'ROUGE_L': 0.6, 'CIDEr': 0.0}\n"
=======
      "{'testlen': 5, 'reflen': 5, 'guess': [5, 4, 3, 2], 'correct': [3, 1, 0, 0]}\n",
      "ratio: 0.9999999998000002\n"
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
     ]
    }
   ],
   "source": [
    "# Quantatitive Eval\n",
    "base_metrics = calc_scores(gts, base_dict)\n",
    "#cap_metrics = calc_scores(gts, cap_dict)\n",
    "#print(base_metrics, cap_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b60204f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bleu_1': 0.5999999997600003,\n",
       " 'Bleu_2': 0.38729833445614026,\n",
       " 'Bleu_3': 3.6840314969416437e-06,\n",
       " 'Bleu_4': 1.2574334290280229e-08,\n",
       " 'ROUGE_L': 0.6,\n",
       " 'CIDEr': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aab426",
   "metadata": {},
   "source": [
    "### Qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65679ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "image = Image.open(sample_path)\n",
    "# Transpose with respect to EXIF data\n",
    "image = ImageOps.exif_transpose(image)\n",
    "w, h = image.size\n",
    "print(\"PIL Image width: {}, height: {}\".format(w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e382b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 9), constrained_layout=True)\n",
    "\n",
    "ax.set_xlabel(\"Trans: \" + base_dict[sample_name][0].capitalize() +\n",
    "              '\\n' + \"EgoTrans: \" + cap_dict[sample_name][0].capitalize() +\n",
    "              '\\n' + \"GT: \" + gts[sample_name][0] +\n",
    "              '\\n' + \"      \" + gts[sample_name][1] +\n",
    "              '\\n' + \"      \" + gts[sample_name][2],\n",
    "              fontsize=23, horizontalalignment='left', x=-0.1,\n",
    "              #fontdict=dict(weight='bold')\n",
    "             )\n",
    "\n",
    "plt.imshow(image)\n",
    "#ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "fig.savefig(join('images', 'EgoEval', sample_path.split(\"/\")[-1]), facecolor='w', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e8796",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Load DeepDiary samples ###\n",
    "amt_data_dir = \"/Users/zhuangzhuangdai/repos/EgoTransformer/images/amt_data\"\n",
    "diary_file = join(amt_data_dir, 'amt_list.txt')\n",
    "\n",
    "diary_tuple = get_deepdiary(269, diary_file)\n",
    "sample_path = join(amt_data_dir, diary_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182071d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dict = predict_qualitative(config_t, sample_path, tags=None, checkpoint_path=baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef621bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap_dict = predict_qualitative(config_ego, sample_path, tags=('indoor', 'daytime'), checkpoint_path=egotrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b97be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 9), constrained_layout=True)\n",
    "\n",
    "ax.set_xlabel(\"Trans: \" + base_dict[next(iter(base_dict))][0].capitalize() +\n",
    "              '\\n' + \"EgoTrans: \" + cap_dict[next(iter(cap_dict))][0].capitalize() +\n",
    "              '\\n' + \"GT: \" + diary_tuple[1][0] +\n",
    "              '\\n' + \"      \" + diary_tuple[1][1],\n",
    "              #'\\n' + \"      \" + diary_tuple[1][2],\n",
    "              fontsize=23, horizontalalignment='left', x=-0.1,\n",
    "              #fontdict=dict(weight='bold')\n",
    "             )\n",
    "\n",
    "plt.imshow(diary_tuple[0])\n",
    "#ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "fig.savefig(join('images', 'deepdiary', diary_tuple[2]), facecolor='w', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6003a",
   "metadata": {},
   "source": [
    "### Quantitative"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 8,
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   "id": "ca6d39d1",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "eval_split = ['03', '12', '20']\n",
=======
    "eval_split = ['02', ]\n",
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
    "\n",
    "config_t = Config()\n",
    "config_ego = ConfigEgo()\n",
    "\n",
    "baseline = baseline = 'finetuneBaseline-best_epoch22_loss12.pth'  #'finetune-best_epoch19_loss15.pth'\n",
    "egotrans = 'finetuneEgoTrans-best_epoch14_loss11.pth'  #'finetuneContextFuse-epoch99_loss16.pth'\n",
    "\n",
<<<<<<< HEAD
    "egotrans_freezebackbone = '9 - finetuneContextFuse-epoch54_loss15.pth'\n",
    "egotrans_freezeencoder = '10 - finetuneContextFuse-epoch19_loss15.pth'\n",
    "egotrans_freezeencoder2 = 'finetuneEgoTransFreezeEncoder-epoch79_loss16.pth'"
=======
    "egotrans_freezebackbone = '9 - finetuneContextFuse-epoch54_loss15.pth'  # Golden model for EgoTrans\n",
    "egotrans_freezeencoder = '10 - finetuneContextFuse-epoch19_loss15.pth'"
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 9,
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   "id": "02b8261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loop_quantitative_eval(config, checkpoint, annotations, split_lst, egocap_dir_path=egocap_dir):\n",
    "    hypo = {}\n",
    "    refs = {}\n",
    "    sample_path_lst, tags_lst = [], []\n",
    "    \n",
    "    for split in split_lst:\n",
    "        for idx, (key, val) in enumerate(annotations.items()):\n",
    "            if int(split) == int(val['SplitIndex']):\n",
    "                sample_name = key\n",
    "                sample_path = join(egocap_dir_path, 'static', 'Split' + split.zfill(2), sample_name)\n",
    "                \n",
    "                if config.modality == 'ego':\n",
    "                    tags = (val['tag_stats']['where']['majority'], val['tag_stats']['when']['majority'])\n",
    "                else:\n",
    "                    tags = None\n",
    "                    \n",
    "                sample_path_lst.append(sample_path)\n",
    "                tags_lst.append(tags)\n",
    "                \n",
    "                ref = {sample_name: val['captions']}\n",
    "                refs.update(ref)\n",
    "                \n",
    "    # Inference with lists of sample_path & tags\n",
    "    pred_dict = predict_qualitative(config, sample_path_lst, tags_lst, checkpoint_path=checkpoint)\n",
    "    hypo.update(pred_dict)\n",
    "                \n",
    "    metrics = calc_scores(refs, hypo)\n",
    "    print(metrics)\n",
    "    return hypo, refs, metrics\n",
    "\n",
    "def quantitative_eval(config, checkpoint, annotations, split_lst, egocap_dir_path=egocap_dir):\n",
    "    hypo = {}\n",
    "    refs = {}\n",
    "    for split in split_lst:\n",
    "        for idx, (key, val) in enumerate(annotations.items()):\n",
    "            if int(split) == int(val['SplitIndex']):\n",
    "                sample_name = key\n",
    "                sample_path = join(egocap_dir_path, 'static', 'Split' + split.zfill(2), sample_name)\n",
    "                \n",
    "                if config.modality == 'ego':\n",
    "                    tags = (val['tag_stats']['where']['majority'], val['tag_stats']['when']['majority'])\n",
    "                else:\n",
    "                    tags = None\n",
    "                \n",
    "                ref = {sample_name: val['captions']}\n",
    "                refs.update(ref)\n",
    "                # Inference\n",
    "                pred_dict = predict_qualitative(config, sample_path, tags=tags, checkpoint_path=checkpoint)\n",
    "                hypo.update(pred_dict)\n",
    "                \n",
    "    metrics = calc_scores(refs, hypo)\n",
    "    print(metrics)\n",
    "    return hypo, refs, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b43d8d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "tuples0 = Loop_quantitative_eval(config_t, baseline, ana, eval_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60e34380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Transformer eval report:  {'Bleu_1': 0.5329520296944237, 'Bleu_2': 0.3710939788537185, 'Bleu_3': 0.2808974177232012, 'Bleu_4': 0.20717381483024427, 'ROUGE_L': 0.4633711682666788, 'CIDEr': 0.5254025645891273}\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# SOTA\n",
    "print(\"Baseline Transformer eval report: \", tuples0[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77bd4e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Transformer eval report:  {'Bleu_1': 0.6062992125974704, 'Bleu_2': 0.4320192308426825, 'Bleu_3': 0.3203847350488713, 'Bleu_4': 0.22426836849150034, 'ROUGE_L': 0.5115384513016008, 'CIDEr': 0.5867149148718238}\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Transformer eval report: \", tuples0[2])"
=======
    "%%capture\n",
    "tuples0 = Loop_quantitative_eval(config_t, baseline, ana, eval_split)"
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba01de6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Transformer eval report:  {'Bleu_1': 0.5914796944031911, 'Bleu_2': 0.42201907242637016, 'Bleu_3': 0.30150299245067447, 'Bleu_4': 0.21249510647796122, 'ROUGE_L': 0.49803407617617435, 'CIDEr': 0.5631176567263839}\n"
     ]
    }
   ],
   "source": [
    "# Benchmark 1\n",
    "print(\"Baseline Transformer eval report: \", tuples0[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32cba1d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "tuples2 = Loop_quantitative_eval(config_ego, egotrans, ana, eval_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b9e989d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egotrans_backbone_grad (PostFuse) eval report:  {'Bleu_1': 0.6386013773052872, 'Bleu_2': 0.5260539192513851, 'Bleu_3': 0.44091675132286035, 'Bleu_4': 0.36591127335976587, 'ROUGE_L': 0.5513383817032499, 'CIDEr': 1.13634268881571}\n"
     ]
    }
   ],
   "source": [
    "# SOTA\n",
    "print(\"Egotrans_backbone_grad (PostFuse) eval report: \", tuples2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fea0936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egotrans_backbone_grad (PostFuse) eval report:  {'Bleu_1': 0.6077519379835539, 'Bleu_2': 0.4426120045846006, 'Bleu_3': 0.327297695245264, 'Bleu_4': 0.22837751360645495, 'ROUGE_L': 0.5155280090633964, 'CIDEr': 0.5759595140290152}\n"
     ]
    }
   ],
   "source": [
    "print(\"Egotrans_backbone_grad (PostFuse) eval report: \", tuples2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "tuples2 = Loop_quantitative_eval(config_ego, egotrans_prefuse, ana, eval_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a382596",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Egotrans_prefuse Transformer eval report: \", tuples[2])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 10,
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   "id": "ccf3cd6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
<<<<<<< HEAD
    "tuples2 = Loop_quantitative_eval(config_ego, egotrans_freezeencoder2, ana, eval_split)"
=======
    "tuples3 = Loop_quantitative_eval(config_ego, egotrans_freezebackbone, ana, eval_split)"
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 11,
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   "id": "7057e42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Egotrans_freezeencoder2 eval report:  {'Bleu_1': 0.6021840873625551, 'Bleu_2': 0.43714872672052174, 'Bleu_3': 0.3228368521246112, 'Bleu_4': 0.22456447512769356, 'ROUGE_L': 0.5115431114476937, 'CIDEr': 0.5674110346347}\n"
=======
      "Egotrans_freezeencoder eval report:  {'Bleu_1': 0.5921058598654757, 'Bleu_2': 0.4275474115928319, 'Bleu_3': 0.314246754523848, 'Bleu_4': 0.23162427307414943, 'ROUGE_L': 0.490402296838267, 'CIDEr': 0.8309363032333273}\n"
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(\"Egotrans_freezeencoder2 eval report: \", tuples2[2])"
=======
    "print(\"Egotrans_freezeencoder eval report: \", tuples3[2])"
>>>>>>> bc2d948774c9830242b9ac9ae7f5577b9eaa9825
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83ac03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
